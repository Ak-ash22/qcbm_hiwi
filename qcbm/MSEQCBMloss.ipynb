{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce5907-7dbb-4407-a507-089b66bce916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# Loss Functions\n",
    "\n",
    "def mse_loss(px, py):\n",
    "    \"\"\"Mean Squared Error Loss\"\"\"\n",
    "    return jnp.mean((px - py) ** 2)\n",
    "\n",
    "\n",
    "def mae_loss(px, py):\n",
    "    \"\"\"Mean Absolute Error Loss\"\"\"\n",
    "    return jnp.mean(jnp.abs(px - py))\n",
    "\n",
    "\n",
    "def kl_divergence_loss(px, py):\n",
    "    \"\"\"Kullback-Leibler Divergence Loss\"\"\"\n",
    "    return jnp.sum(py * jnp.nan_to_num(jnp.log(py / px)))\n",
    "\n",
    "\n",
    "def cross_entropy_loss(px, py):\n",
    "    \"\"\"Cross-Entropy Loss\"\"\"\n",
    "    return -jnp.sum(py * jnp.nan_to_num(jnp.log(px)))\n",
    "\n",
    "\n",
    "def rmse_loss(px, py):\n",
    "    \"\"\"Root Mean Squared Error Loss\"\"\"\n",
    "    return jnp.sqrt(jnp.mean((px - py) ** 2))\n",
    "\n",
    "\n",
    "# Kernel Functions\n",
    "def gaussian_kernel(x1, x2, bandwidth=1.0):\n",
    "    \"\"\"Gaussian Kernel\"\"\"\n",
    "    dist = jnp.sum((x1 - x2) ** 2)\n",
    "    return jnp.exp(-dist / (2 * bandwidth ** 2))\n",
    "\n",
    "\n",
    "def linear_kernel(x1, x2):\n",
    "    \"\"\"Linear Kernel\"\"\"\n",
    "    return jnp.dot(x1, x2)\n",
    "\n",
    "\n",
    "def polynomial_kernel(x1, x2, degree=3):\n",
    "    \"\"\"Polynomial Kernel\"\"\"\n",
    "    return (1 + jnp.dot(x1, x2)) ** degree\n",
    "\n",
    "\n",
    "# MMD Loss Function with Kernel Choice\n",
    "def mmd_loss(px, py, kernel, **kernel_params):\n",
    "    \"\"\"Maximum Mean Discrepancy (MMD) Loss with a chosen kernel\"\"\"\n",
    "    kpxpx = jnp.mean([kernel(x, y, **kernel_params) for x in px for y in px])\n",
    "    kpypy = jnp.mean([kernel(x, y, **kernel_params) for x in py for y in py])\n",
    "    kpxpy = jnp.mean([kernel(x, y, **kernel_params) for x in px for y in py])\n",
    "    return kpxpx + kpypy - 2 * kpxpy\n",
    "\n",
    "\n",
    "def learn_qcbm(target_data, n_layers=3, n_iterations=100, learning_rate=0.1, loss_function=\"mse\", kernel=None, kernel_params=None):\n",
    "    \"\"\"\n",
    "    Train a QCBM to learn a target distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - target_data: The target probability distribution or function data (will be normalized).\n",
    "    - n_layers: The number of strongly entangling layers in the quantum circuit.\n",
    "    - n_iterations: The number of training iterations.\n",
    "    - learning_rate: The learning rate for the Adam optimizer.\n",
    "    - loss_function: The loss function to use (options: \"mse\", \"mae\", \"kl\", \"ce\", \"rmse\", \"mmd\"). Default is \"mse\".\n",
    "    - kernel: Optional kernel function to use with MMD (options: \"gaussian\", \"linear\", \"polynomial\"). Default is None.\n",
    "    - kernel_params: Dictionary of parameters for the kernel function (e.g., bandwidth for Gaussian kernel, degree for Polynomial kernel).\n",
    "\n",
    "    Returns:\n",
    "    - qcbm_probs: The learned probability distribution from the QCBM.\n",
    "    \"\"\"\n",
    "    # Step 1: Normalize the target data so it sums to 1\n",
    "    target_data = np.array(target_data)\n",
    "    Sum,Min = np.sum(target_data),np.min(target_data)  #For Rescaling after learning\n",
    "    target_data = target_data - np.min(target_data)  # Ensure non-negative values\n",
    "    target_data = target_data / np.sum(target_data)  # Normalize to make it a probability distribution\n",
    "\n",
    "    # Step 2: Determine the number of qubits needed\n",
    "    data_size = len(target_data)\n",
    "    n_qubits = int(np.ceil(np.log2(data_size)))\n",
    "    x_max = 2 ** n_qubits  # This is the total number of possible basis states\n",
    "\n",
    "    # Pad the target data if necessary to match 2^n_qubits\n",
    "    if data_size < x_max:\n",
    "        target_data = np.pad(target_data, (0, x_max - data_size), 'constant')\n",
    "\n",
    "    # Define the device and the QCBM circuit using PennyLane\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "    @qml.qnode(dev, interface=\"jax\")\n",
    "    def circuit(weights):\n",
    "        qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "        return qml.probs(wires=range(n_qubits))\n",
    "\n",
    "    # JIT compile the circuit for speed\n",
    "    jit_circuit = jax.jit(circuit)\n",
    "\n",
    "    # Choose the loss function\n",
    "    if loss_function == \"mse\":\n",
    "        loss_fn = mse_loss\n",
    "    elif loss_function == \"mae\":\n",
    "        loss_fn = mae_loss\n",
    "    elif loss_function == \"kl\":\n",
    "        loss_fn = kl_divergence_loss\n",
    "    elif loss_function == \"ce\":\n",
    "        loss_fn = cross_entropy_loss\n",
    "    elif loss_function == \"rmse\":\n",
    "        loss_fn = rmse_loss\n",
    "    elif loss_function == \"mmd\":\n",
    "        if kernel is None:\n",
    "            raise ValueError(\"MMD loss requires a kernel to be specified.\")\n",
    "        # Wrap the loss function with the chosen kernel\n",
    "        loss_fn = lambda px, py: mmd_loss(px, py, kernel, **kernel_params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss function '{loss_function}'. Supported options are: 'mse', 'mae', 'kl', 'ce', 'rmse', 'mmd'.\")\n",
    "\n",
    "    # Define the QCBM class to compute the loss\n",
    "    class QCBM:\n",
    "        def __init__(self, circ, py, loss_fn):\n",
    "            self.circ = circ\n",
    "            self.py = py  # Target distribution Ï€(x)\n",
    "            self.loss_fn = loss_fn\n",
    "\n",
    "        # General loss function (e.g., MSE, MAE, KL Divergence, MMD with kernel)\n",
    "        @partial(jax.jit, static_argnums=0)\n",
    "        def loss(self, params):\n",
    "            px = self.circ(params)  # Get probabilities from QCBM circuit\n",
    "            return self.loss_fn(px, self.py), px  # Return the loss and probabilities\n",
    "\n",
    "    # Initialize QCBM with the compiled circuit and target distribution\n",
    "    qcbm = QCBM(jit_circuit, target_data, loss_fn)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    opt = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Randomly initialize the weights for the Strongly Entangling Layers\n",
    "    wshape = qml.StronglyEntanglingLayers.shape(n_layers=n_layers, n_wires=n_qubits)\n",
    "    weights = np.random.random(size=wshape)\n",
    "\n",
    "    # Initialize the optimizer state\n",
    "    opt_state = opt.init(weights)\n",
    "\n",
    "    # Define the update step for gradient descent\n",
    "    @jax.jit\n",
    "    def update_step(params, opt_state):\n",
    "        # Compute the loss and gradients\n",
    "        (loss_val, qcbm_probs), grads = jax.value_and_grad(qcbm.loss, has_aux=True)(params)\n",
    "        # Update the optimizer state and parameters\n",
    "        updates, opt_state = opt.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, loss_val\n",
    "\n",
    "    # Training loop\n",
    "    for i in range(n_iterations):\n",
    "        weights, opt_state, loss_val = update_step(weights, opt_state)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration {i}, Loss: {loss_val:.4f}\")\n",
    "\n",
    "    # Get the final QCBM probabilities after training\n",
    "    qcbm_probs = qcbm.circ(weights)\n",
    "\n",
    "    return qcbm_probs,Sum*qcbm_probs + Min\n",
    "\n",
    "norm_res,res=learn_qcbm(target_data=silu(np.linspace(-2.5,2.5,num=64)), n_layers=10, n_iterations=150, learning_rate=0.1, loss_function=\"rmse\", kernel=gaussian_kernel,kernel_params={\"bandwidth\": 1.0})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pennylane",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
