{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    \n",
    "    def __init__(self,n_in,n_weights_per_edge,weights_range=None):\n",
    "        self.n_in = n_in     # of inputs\n",
    "        self.n_weights_per_edge = n_weights_per_edge  # of weights per edge\n",
    "        weights_range = [-1,1] if weights_range is None else weights_range\n",
    "        self.weights = np.random.uniform(weights_range[0], weights_range[-1], size=(self.n_in, self.n_weights_per_edge))\n",
    "        \n",
    "        #variables\n",
    "        self.bias = 0\n",
    "        self.xin = None     #input variable\n",
    "        self.xmid = None\n",
    "        self.xout = None\n",
    "        \n",
    "        #derivatives\n",
    "        self.dxout_dxmid = None\n",
    "        self.dxout_dbias = None\n",
    "        self.dxmid_dw = None\n",
    "        self.dxmid_dxin = None\n",
    "        self.dxout_dxin = None\n",
    "        self.dxout_dw = None\n",
    "        \n",
    "        #derivatives for loss function\n",
    "        self.dloss_dw = np.zeros((self.n_in, self.n_weights_per_edge)) #product chain rule with this array size\n",
    "        self.dloss_dbias = 0 \n",
    "        \n",
    "    def __call__(self,xin):\n",
    "        \n",
    "        #compute neuron's output\n",
    "        self.xin = xin     #input variables\n",
    "        self.get_xmid()\n",
    "        self.get_xout()\n",
    "        \n",
    "        #compute derivates\n",
    "        self.get_dxout_dxmid()\n",
    "        self.get_dxout_dbias()\n",
    "        self.get_dxmid_dw()\n",
    "        self.get_dxmid_dxin()\n",
    "        \n",
    "        #check for AssertionError\n",
    "        assert self.dxout_dxmid.shape == (self.n_in, )\n",
    "        assert self.dxmid_dxin.shape == (self.n_in, )\n",
    "        #assert self.dxmid_dw.shape == (self.n_in, self.n_weights_per_edge)\n",
    "\n",
    "        self.get_dxout_dxin()\n",
    "        self.get_dxout_dw()\n",
    "        \n",
    "        return self.xout\n",
    "    \n",
    "    def get_xmid(self):\n",
    "        pass\n",
    " \n",
    "    def get_xout(self):\n",
    "        pass\n",
    " \n",
    "    def get_dxout_dxmid(self):\n",
    "        pass\n",
    " \n",
    "    def get_dxout_dbias(self):\n",
    "        pass  #self.dxout_dbias = 0  # by default\n",
    " \n",
    "    def get_dxmid_dw(self):\n",
    "        pass\n",
    " \n",
    "    def get_dxmid_dxin(self):\n",
    "        pass\n",
    " \n",
    "    def get_dxout_dxin(self):\n",
    "        self.dxout_dxin = self.dxout_dxmid * self.dxmid_dxin\n",
    " \n",
    "    def get_dxout_dw(self):\n",
    "        self.dxout_dw = self.dxout_dxmid *self.dxmid_dw  #making dimensions same and doing matrix multiplication\n",
    "    \n",
    "    def update_dloss_dw_dbias(self,dloss_dxout):\n",
    "        self.dloss_dw += self.dxout_dw * dloss_dxout\n",
    "        self.dloss_dbias += self.dxout_dbias * dloss_dxout\n",
    "    \n",
    "    #finding optimal weights and bias via gradient descent method    \n",
    "    def gradient_descent(self,eps):\n",
    "        self.weights -= eps * self.dloss_dw\n",
    "        self.bias -= eps * self.dloss_dbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classic Neuron\n",
    "\n",
    "#activation functions\n",
    "def relu(x,get_derivative=False):\n",
    "    return x*(x>0) if get_derivative is False else 1.0*(x>=0)\n",
    "\n",
    "def tanh_act(x, get_derivative=False):\n",
    "    if not get_derivative:\n",
    "        return math.tanh(x)\n",
    "    return 1 - math.tanh(x) ** 2\n",
    " \n",
    "def sigmoid_act(x, get_derivative=False):\n",
    "    if not get_derivative:\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    return sigmoid_act(x) * (1 - sigmoid_act(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronNN(Neuron):\n",
    "    \n",
    "    def __init__(self, n_in, weights_range=None, activation=relu):\n",
    "        super().__init__(n_in, n_weights_per_edge = 1, weights_range=weights_range)\n",
    "        self.activation = activation\n",
    "        self.activation_input = None     #Input variables for activation function\n",
    "        \n",
    "    def get_xmid(self):\n",
    "        self.xmid = self.weights[:,0] * self.xin\n",
    "        \n",
    "    def get_xout(self):\n",
    "        self.activation_input = sum(self.xmid.flatten()) + self.bias\n",
    "        self.xout = self.activation(self.activation_input,get_derivative=False)\n",
    "        \n",
    "    def get_dxout_dxmid(self):\n",
    "        self.dxout_dxmid = self.activation(self.activation_input,get_derivative=True) * np.ones((self.n_in))\n",
    "        \n",
    "    def get_dxout_dbias(self):\n",
    "        self.dxout_dbias = self.activation(self.activation_input,get_derivative=True)\n",
    "        \n",
    "    def get_dxmid_dw(self):\n",
    "        self.dxmid_dw = np.reshape(self.xin, (-1, 1))\n",
    " \n",
    "    def get_dxmid_dxin(self):\n",
    "        self.dxmid_dxin = self.weights.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KAN Neuron\n",
    "\n",
    "#activation function - BSpline\n",
    "from scipy.interpolate import BSpline\n",
    " \n",
    "def get_bsplines(x_bounds, n_fun, degree=3, **kwargs):\n",
    "    grid_len = n_fun - degree + 1\n",
    "    step = (x_bounds[1] - x_bounds[0]) / (grid_len - 1)\n",
    "    edge_fun, edge_fun_der = {}, {}\n",
    "\n",
    "    # SiLU bias function\n",
    "    edge_fun[0] = lambda x: x / (1 + np.exp(-x))\n",
    "    edge_fun_der[0] = lambda x: (1 + np.exp(-x) + x * np.exp(-x)) / np.power((1 + np.exp(-x)), 2)\n",
    "\n",
    "    # B-splines\n",
    "    t = np.linspace(x_bounds[0] - degree * step, x_bounds[1] + degree * step, grid_len + 2 * degree)\n",
    "    t[degree], t[-degree - 1] = x_bounds[0], x_bounds[1]\n",
    "    for ind_spline in range(n_fun - 1):\n",
    "        edge_fun[ind_spline + 1] = BSpline.basis_element(t[ind_spline:ind_spline + degree + 2], extrapolate=False)\n",
    "        edge_fun_der[ind_spline + 1] = edge_fun[ind_spline + 1].derivative()\n",
    "    return edge_fun, edge_fun_der\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronKAN(Neuron):\n",
    "    \n",
    "    def __init__(self, n_in, n_weights_per_edge,x_bounds, weights_range=None, get_edge_fun=get_bsplines, **kwargs):\n",
    "        self.x_bounds = x_bounds\n",
    "        super().__init__(n_in, n_weights_per_edge=n_weights_per_edge, weights_range=weights_range)\n",
    "        self.edge_fun, self.edge_fun_der = get_edge_fun(self.x_bounds,self.n_weights_per_edge, **kwargs)\n",
    "        \n",
    "    def get_xmid(self):\n",
    "        self.phi_x_mat = np.array([[self.edge_fun[b](self.xin)] for b in self.edge_fun]).T\n",
    "        self.phi_x_mat[np.isnan(self.phi_x_mat)] = 0\n",
    "        self.xmid = (self.weights * self.phi_x_mat).sum(axis=1)\n",
    "        print(np.shape(self.phi_x_mat))\n",
    "        \n",
    "    def get_xout(self):\n",
    "       # note: node function <- tanh to avoid any update of spline grids\n",
    "        self.xout = tanh_act(sum(self.xmid.flatten()), get_derivative=False)\n",
    "        \n",
    "    def get_dxout_dxmid(self):\n",
    "        self.dxout_dxmid = tanh_act(sum(self.xmid.flatten()), get_derivative=True) * np.ones(self.n_in)\n",
    "        print(self.n_in) \n",
    "        \n",
    "    def get_dxmid_dw(self):\n",
    "        self.dxmid_dw = self.phi_x_mat\n",
    "        \n",
    "    def get_dxmid_dxin(self):\n",
    "        phi_x_der_mat = np.array([self.edge_fun_der[b](self.xin) if self.edge_fun[b](self.xin) is not None else 0\n",
    "                                  for b in self.edge_fun_der]).T  # shape (n_in, n_weights_per_edge)\n",
    "        phi_x_der_mat[np.isnan(phi_x_der_mat)] = 0\n",
    "        self.dxmid_dxin = (self.weights * phi_x_der_mat).sum(axis=1)\n",
    "        \n",
    "    def get_dxout_dbias(self):\n",
    "        # no bias in KAN!\n",
    "        self.dxout_dbias = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    \n",
    "    def __init__(self, n_in, n_out, neuron_class=NeuronNN, **kwargs):\n",
    "        self.n_in, self.n_out = n_in, n_out\n",
    "        self.neurons = [neuron_class(n_in) if (kwargs=={}) else neuron_class(n_in,**kwargs) for _ in range(n_out)]\n",
    "        \n",
    "        self.xin = None\n",
    "        self.xout = None\n",
    "        self.dloss_dxin = None\n",
    "        self.zero_grad()\n",
    "        \n",
    "    def __call__(self, xin):\n",
    "        self.xin = xin\n",
    "        self.out = np.array([nn(self.n_in) for nn in self.neurons])\n",
    "        return self.xout\n",
    "    \n",
    "    def zero_grad(self,which=None):\n",
    "        \n",
    "        #reset gradients to zero\n",
    "        if which is None:\n",
    "            which = ['xin','weights','bias']\n",
    "        \n",
    "        for w in which:\n",
    "            if w == 'xin':\n",
    "                self.dloss_dxin = np.zeros(self.n_in)\n",
    "            elif w == 'weights':\n",
    "                self.dloss_dw = np.zeros((self.n_in,self.neurons[0].n_weights_per_edge))\n",
    "            elif w == 'bias':\n",
    "                self.dloss_dbias = 0\n",
    "            else:\n",
    "                raise ValueError('input \\'which\\' value is not recognised')\n",
    "            \n",
    "    def update_grad(self,dloss_dxout):\n",
    "        \n",
    "        #upgrading new loss gradients by chain rule\n",
    "        for ii, dloss_dxout_tmp in enumerate(dloss_dxout):\n",
    "            self.dloss_dxin += self.neurons[ii].dxout_dxin * dloss_dxout_tmp\n",
    "            \n",
    "            self.neurons[ii].update_dloss_dw_dbias(dloss_dxout_tmp)\n",
    "            \n",
    "        return self.dloss_dxin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class Loss:\n",
    "    \n",
    "    def __init__(self, n_in):\n",
    "        self.n_in = n_in\n",
    "        self.y, self.dloss_dy, self.loss, self.y_train = None,None,None,None\n",
    "        \n",
    "    def __call__(self,y,y_train):\n",
    "        # y: output of network\n",
    "        # y_train: ground truth\n",
    "        self.y, self.y_train = np.array(y), y_train\n",
    "        self.get_loss()\n",
    "        self.get_dloss_dy()\n",
    "        return self.loss\n",
    "    \n",
    "    def get_loss(self):\n",
    "        # compute loss l(y, y_train)\n",
    "        pass\n",
    " \n",
    "    def get_dloss_dy(self):\n",
    "        # compute gradient of loss wrt y\n",
    "        pass\n",
    "     \n",
    "\n",
    "#For Regression Tasks\n",
    "class SquaredLoss(Loss):\n",
    " \n",
    "    def get_loss(self):\n",
    "        # compute loss l(xin, y)\n",
    "        self.loss = np.mean(np.power(self.y - self.y_train, 2))\n",
    " \n",
    "    def get_dloss_dy(self):\n",
    "        # compute gradient of loss wrt xin\n",
    "        self.dloss_dy = 2 * (self.y - self.y_train) / self.n_in\n",
    "\n",
    "\n",
    "#For Classification Tasks        \n",
    "class CrossEntropyLoss(Loss):\n",
    " \n",
    "    def get_loss(self):\n",
    "        # compute loss l(xin, y)\n",
    "        self.loss = - np.log(np.exp(self.y[self.y_train[0]]) / sum(np.exp(self.y)))\n",
    " \n",
    "    def get_dloss_dy(self):\n",
    "        # compute gradient of loss wrt xin\n",
    "        self.dloss_dy = np.exp(self.y) / sum(np.exp(self.y))\n",
    "        self.dloss_dy[self.y_train] -= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class FeedForward:\n",
    "    \n",
    "    def __init__(self, layer_len, eps=0.0001, seed=None, loss=SquaredLoss, **kwargs):\n",
    "        self.seed = np.random.randint(int(1e4)) if seed is None else int(seed)\n",
    "        np.random.seed(self.seed)\n",
    "        self.layer_len = layer_len\n",
    "        self.eps = eps\n",
    "        self.n_layers = np.shape(layer_len)[0] - 1\n",
    "        self.layers = [FullyConnectedLayer(layer_len[ii], layer_len[ii+1], **kwargs) for ii in range(self.n_layers)]\n",
    "        self.loss = loss(self.layer_len[-1])\n",
    "        self.loss_hist = None\n",
    "        \n",
    "    def __call__(self,x):\n",
    "    \n",
    "        #forward pass\n",
    "        x_in = x\n",
    "        \n",
    "        #call each layer of fully connected network with previous input to the next layer\n",
    "        for ll in range(self.n_layers):\n",
    "             x_in = self.layers[ll](x_in)\n",
    "        return x_in\n",
    "        \n",
    "    def backprop(self):\n",
    "        \n",
    "        #gradient backpropogation\n",
    "        delta = self.layer[-1].update_grad(self.loss.dloss_dy)\n",
    "        for ll in range(self.layers-1)[::-1]:\n",
    "            delta = self.layers[ll].update_grad(delta)\n",
    "            \n",
    "    def gradient_descent_par(self):\n",
    "        #update parameters via gradient descent\n",
    "        for ll in self.layers:\n",
    "            for nn in ll.neurons:\n",
    "                nn.gradient_descent(self.eps)\n",
    "             \n",
    "                \n",
    "    def train(self, x_train, y_train, n_iter_max=10000, loss_tol=0.1):\n",
    "        self.loss_hist = np.zeros(n_iter_max)\n",
    "        x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "        \n",
    "        assert x_train.shape[0] == y_train.shape[0], 'x_train, y_train must contain the same number of samples'\n",
    "        assert x_train.shape[1] == self.layer_len[0], 'shape of x_train is incompatible with first layer'\n",
    "        \n",
    "        #progress bar\n",
    "        pbar = tqdm(range(n_iter_max))\n",
    "        for it in pbar:\n",
    "            loss = 0  #reset loss\n",
    "            \n",
    "            for ii in range(x_train.shape[0]):\n",
    "                x_out = self(x_train[ii,:])            #forward pass\n",
    "                loss += self.loss(x_out,y_train[ii,:])  #accumulate loss\n",
    "                self.backprop()                         #backpropogation\n",
    "                [layer.zero_grad(which=['xin']) for layer in self.layers] #reset gradient wrt xin to zero\n",
    "            self.loss_hist[it] = loss\n",
    "            \n",
    "            if (it%10) == 0:\n",
    "                pbar.set_postfix_str(f'loss: {loss:.3f}')\n",
    "            if loss < loss_tol:\n",
    "                pbar.set_postfix_str(f'loss: {loss:.3f}. Convergence has been attained!')\n",
    "                self.loss_hist = self.loss_hist[: it]\n",
    "                break\n",
    "            self.gradient_descent_par()  # update parameters\n",
    "            [layer.zero_grad(which=['weights', 'bias']) for layer in self.layers]  # reset gradient wrt par to zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
